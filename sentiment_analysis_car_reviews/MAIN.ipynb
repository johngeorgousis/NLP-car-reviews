{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b6fcea-e955-4f5f-bfc5-3fb3ba4a43c3",
   "metadata": {},
   "source": [
    "https://towardsai.net/p/nlp/natural-language-processing-nlp-with-python-tutorial-for-beginners-1f54e610a1a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcded58f-4164-469c-b9c2-14a0bb3d4d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from IPython.display import display\n",
    "\n",
    "# NLP\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81daae-bf51-4dc9-95ad-0074e183f9f4",
   "metadata": {},
   "source": [
    "### ðŸ”´ 1. Exploration\n",
    "- Import and shuffle dataset\n",
    "- Change labels Neg: 0, Pos: 1\n",
    "- Print examples in each class\n",
    "- Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae710a14-47d6-4740-a947-c8055e3fe46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0</td>\n",
       "      <td>That 2001 Ford Explorer you may be considerin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>1</td>\n",
       "      <td>Get an old Tempo and stick to your beat   Yes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0</td>\n",
       "      <td>My wife and I lease a 1997 FORD RANGER XLT  S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>1</td>\n",
       "      <td>I wanted a Ford Mustang Convertible for years...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>1</td>\n",
       "      <td>We traded in our Ford Explorer on an 1998 Edd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentiment                                             Review\n",
       "309          0   That 2001 Ford Explorer you may be considerin...\n",
       "741          1   Get an old Tempo and stick to your beat   Yes...\n",
       "265          0   My wife and I lease a 1997 FORD RANGER XLT  S...\n",
       "823          1   I wanted a Ford Mustang Convertible for years...\n",
       "778          1   We traded in our Ford Explorer on an 1998 Edd..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Balance: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    691\n",
       "1    691\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows: 1382\n",
      "Train rows: 1105\n",
      "Test rows: 277\n"
     ]
    }
   ],
   "source": [
    "# Import, shuffle, numeric labels\n",
    "df = pd.read_csv('car_reviews (1).csv')\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "df['Sentiment'] = df.Sentiment.map({'Neg': 0, 'Pos': 1})\n",
    "display(df.head())\n",
    "\n",
    "# Class balance\n",
    "print('Class Balance: ')\n",
    "display(df.Sentiment.value_counts())\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Review'], df['Sentiment'], train_size=0.8, random_state=1)\n",
    "\n",
    "print(f'\\nTotal rows: {df.shape[0]}')\n",
    "print(f'Train rows: {X_train.shape[0]}')\n",
    "print(f'Test rows: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb27c7d8-45f4-4e60-95c2-3f86d809ea3e",
   "metadata": {},
   "source": [
    "### ðŸ”´ 2. Preprocessing (Bag of Words)\n",
    "\n",
    "- Transform and fit training data\n",
    "- Transform testing data\n",
    "\n",
    "**Notes on CountVectorizer** \n",
    "- Implemented with stemming. Stemming turns similar words e.g. studying, study, studied into a single word: 'studi'\n",
    "\n",
    "- Converts tokenized words to lower case with parameter `lowercase = True`. 'He' and 'he' are not treated differently. \n",
    "\n",
    "- Ignores all punctuation using parameter `token_pattern` which has a default regular expression which selects tokens of 2 or more alphanumeric characters.\n",
    "\n",
    "- Ignores all stop English stop words with parameter `stop_words='english'` (stop words are the most commonly used words e.g. 'am, 'and', 'the'). \n",
    "This is extremely helpful as stop words can skew our calculations when we are trying to find certain key words that are indicative of sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ae20bdb-9344-42d3-ba3e-6d2f53343acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of the first 5 rows for visualisation: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>01</th>\n",
       "      <th>10</th>\n",
       "      <th>101k</th>\n",
       "      <th>11</th>\n",
       "      <th>112</th>\n",
       "      <th>120</th>\n",
       "      <th>15</th>\n",
       "      <th>1500</th>\n",
       "      <th>18</th>\n",
       "      <th>...</th>\n",
       "      <th>writer</th>\n",
       "      <th>writing</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yo</th>\n",
       "      <th>yuk</th>\n",
       "      <th>yyvonne</th>\n",
       "      <th>zetec</th>\n",
       "      <th>zone</th>\n",
       "      <th>zx2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1008 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  01  10  101k  11  112  120  15  1500  18  ...  writer  writing  year  \\\n",
       "0   0   0   0     0   0    0    0   0     0   0  ...       0        0     0   \n",
       "1   0   0   0     0   0    0    0   1     0   0  ...       0        0     0   \n",
       "2   0   0   0     0   0    0    0   0     0   1  ...       0        0     0   \n",
       "3   3   1   1     0   0    1    1   1     0   0  ...       1        1     2   \n",
       "4   0   0   0     1   1    0    0   0     1   0  ...       0        0     0   \n",
       "\n",
       "   years  yo  yuk  yyvonne  zetec  zone  zx2  \n",
       "0      1   0    0        0      1     0    1  \n",
       "1      1   0    0        0      0     0    0  \n",
       "2      0   2    1        0      0     0    0  \n",
       "3      4   0    0        1      0     1    0  \n",
       "4      2   0    0        0      0     0    0  \n",
       "\n",
       "[5 rows x 1008 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johng\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# (Optional) Visualisation example\n",
    "print('Example of the first 5 rows for visualisation: ')\n",
    "count_vector = CountVectorizer(lowercase=True, stop_words='english')\n",
    "count_vector.fit(X_train[0:5])\n",
    "frequency_matrix = pd.DataFrame(count_vector.transform(X_train[0:5]).toarray(), columns = count_vector.get_feature_names())\n",
    "display(frequency_matrix)\n",
    "\n",
    "# Stemming function from: https://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "# Stemming turns similar words e.g. studying, study, studied into a single word: 'studi'\n",
    "def textblob_tokenizer(str_input):\n",
    "    blob = TextBlob(str_input.lower())\n",
    "    tokens = blob.words\n",
    "    words = [token.stem() for token in tokens]\n",
    "    return words\n",
    "\n",
    "# Instantiate the CountVectorizer method (for tokenization)\n",
    "count_vector = CountVectorizer(lowercase=True, stop_words='english', tokenizer=textblob_tokenizer)\n",
    "\n",
    "# Transform and fit training data and return matrix\n",
    "X_train = count_vector.fit_transform(X_train)\n",
    "\n",
    "# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n",
    "X_test = count_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eddda9-fd02-4e99-8195-3e25d451be36",
   "metadata": {},
   "source": [
    "### ðŸ”´ 3. Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06e030eb-8ef1-49f9-ba78-f6ef3852a5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Instantiate and fit Naive Bayes\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f398476-c5b7-4f06-84bc-ba2d6a408da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.7689530685920578\n",
      "Precision score:  0.7482517482517482\n",
      "Recall score:  0.7925925925925926\n",
      "F1 score:  0.7697841726618705\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate Naive Bayes\n",
    "predictions = naive_bayes.predict(X_test)\n",
    "print('Accuracy score: ', format(accuracy_score(y_test, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8944cf7d-7cf5-4cc9-9b7d-7d37678b812d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def remove_text_inside_brackets(text, brackets=\"()[]\"):\n",
    "    count = [0] * (len(brackets) // 2) # count open/close brackets\n",
    "    saved_chars = []\n",
    "    for character in text:\n",
    "        for i, b in enumerate(brackets):\n",
    "            if character == b: # found bracket\n",
    "                kind, is_close = divmod(i, 2)\n",
    "                count[kind] += (-1)**is_close # `+1`: open, `-1`: close\n",
    "                if count[kind] < 0: # unbalanced bracket\n",
    "                    count[kind] = 0  # keep it\n",
    "                else:  # found bracket to remove\n",
    "                    break\n",
    "        else: # character is not a [balanced] bracket\n",
    "            if not any(count): # outside brackets\n",
    "                saved_chars.append(character)\n",
    "    return ''.join(saved_chars)\n",
    "\n",
    "def cmx_sklearn(models, test_examples, test_labels, dim=4):\n",
    "    \n",
    "    '''\n",
    "    Plots Confusion Matrix for sklearn list of models\n",
    "    \n",
    "    '''\n",
    "    cmxs = []\n",
    "    \n",
    "    for model in models:\n",
    "            \n",
    "            preds = model.predict(test_examples)\n",
    "            labels = test_labels\n",
    "\n",
    "            cmx_non_normal = tf.math.confusion_matrix(labels, preds).numpy() # Create Confusion Matrix\n",
    "            cmx0 = cmx_non_normal[0] / cmx_non_normal[0].sum()\n",
    "            cmx1 = cmx_non_normal[1] / cmx_non_normal[1].sum()\n",
    "            cmx = np.stack((cmx0, cmx1), axis=0)\n",
    "            cmxs.append(cmx)\n",
    "\n",
    "    plt.figure(figsize=(25,20))\n",
    "    for n in range(len(cmxs)):\n",
    "        # Plot confusion matrix\n",
    "        ax = plt.subplot(dim, dim, n+1)\n",
    "        sns.heatmap(cmxs[n], cmap=['skyblue', 'deepskyblue', 'dodgerblue', 'blue',  'darkblue'])\n",
    "\n",
    "        # xylabels and title\n",
    "        plt.title(remove_text_inside_brackets(str(models[n])))\n",
    "        plt.xlabel('PREDICTIONS')\n",
    "        plt.ylabel('LABELS')\n",
    "\n",
    "        # Label ticks\n",
    "        ax.set_xticklabels(['Background', 'Signal'])\n",
    "        ax.set_yticklabels(['Background', 'Signal'])\n",
    "        # Align ticks\n",
    "        plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "        plt.setp(ax.get_yticklabels(), rotation=90, ha=\"center\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "\n",
    "        # Text Annotations for Blocks in CMX\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "\n",
    "                value = int(np.round(100*cmxs[n][i, j], 0))\n",
    "\n",
    "                text = ax.text(j+0.5, \n",
    "                               i+0.5, \n",
    "                               value,\n",
    "                               ha=\"center\", \n",
    "                               va=\"center\", \n",
    "                               color=\"orangered\", \n",
    "                               fontsize = 20)\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()      \n",
    "    print(cmxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d90792cc-3ead-4454-bdbc-5b0e1bd6630f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.74647887, 0.25352113],\n",
      "       [0.20740741, 0.79259259]])]\n"
     ]
    }
   ],
   "source": [
    "cmx_sklearn([naive_bayes], X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
